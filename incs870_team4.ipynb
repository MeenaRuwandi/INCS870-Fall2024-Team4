{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeenaRuwandi/INCS_870_Project_IDSforMinorAttacks/blob/meena/incs870_team4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAonk6lUz5UI",
        "outputId": "e8f93d5d-7189-4402-8261-de67a2343543"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJzGzg0FzrC0",
        "outputId": "7f75068e-761e-43f1-db77-00bcfa660b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "G2mRgdV89K2S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_ids = [\n",
        "    '1zOeCqtGZjAj_nSLe3W5MH5pDQM28bPde',  # Replace with actual file ID\n",
        "    '1fagr2rKW8EN-Psc5UlD9BFfa0rRRF6i3',  # Replace with actual file ID\n",
        "    '1WL00LFkCA2ylV8_8c6jm52bnEhGLcKsq',   # Replace with actual file ID\n",
        "    '1xr_QBU3Ab42nW1ELl0NUydx4Rbx60TVR']\n",
        "    #'1MpqHsC5wQSu9CxlhJimz25EVcYubSMhu',\n",
        "    #'18mKfJHXinmfwPpaYNU2D6xHwq4dQuEuL',\n",
        "    #'1h1lMHytamf4Kc66NfAfMfBzDbmwM5Ub5',\n",
        "    #'11__JhC64_D4ezxjF8d0dPNRDJF-TY6Pq'\n",
        "#]\n",
        "\n",
        "\n",
        "# Download each file\n",
        "for i, file_id in enumerate(file_ids):\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    gdown.download(url, f'dataset_{i + 1}.csv', quiet=False)  # Saves as dataset_1.csv, dataset_2.csv, etc."
      ],
      "metadata": {
        "id": "yQuMhkmd0NNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = [\n",
        "    '/content/dataset_1.csv','/content/dataset_2.csv','/content/dataset_3.csv','/content/dataset_4.csv']\n",
        "    #'/content/dataset_5.csv','/content/dataset_6.csv','/content/dataset_7.csv','/content/dataset_8.csv',\n",
        "#]\n",
        "\n",
        "# Make the data frame\n",
        "dataframes = [pd.read_csv(path) for path in dataset_paths]\n",
        "\n",
        "# Combine all datasets into one dataframe\n",
        "combined_dataset = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "Z4B6Sgje9cA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset.describe()"
      ],
      "metadata": {
        "id": "M4JSokVCATPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset.shape"
      ],
      "metadata": {
        "id": "d3t3Uy7JAhj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset.info()"
      ],
      "metadata": {
        "id": "ZIxOX8_R1Sin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre processing\n",
        "x = combined_dataset.drop(columns=[' Label'])\n",
        "y = combined_dataset[' Label']\n",
        "# Handle missing values\n",
        "x.fillna(x.mean(), inplace=True)\n",
        "\n",
        "# Get 'Label' column that indicates attack types\n",
        "class_distribution = combined_dataset[' Label'].value_counts()\n",
        "print(class_distribution)"
      ],
      "metadata": {
        "id": "y7Zvt-PyBKnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "missing_values=combined_dataset.isnull().sum()\n",
        "print(\"missing values in each column :\\n\", missing_values[missing_values>0])"
      ],
      "metadata": {
        "id": "uDzdKjeo5uz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset.columns = combined_dataset.columns.str.strip()\n",
        "print(combined_dataset.columns.tolist())\n",
        "\n",
        "\n",
        "#fill missing values with the mean value of 'Flow Bytes/s' column with its mean value\n",
        "meanVal = combined_dataset['Flow Bytes/s'].mean()\n",
        "\n",
        "#verify if all the missing values are handled\n",
        "combined_dataset['Flow Bytes/s'].fillna(meanVal, inplace=True)\n",
        "missingValCheck = combined_dataset.isnull().sum()\n",
        "\n",
        "#print(\"missing values after handling : \\n\", missingValCheck[missingValCheck>0])\n",
        "print(\"Missing values after handling:\\n\", missingValCheck[missingValCheck > 0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9FlSWH438FNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset.columns.tolist())\n",
        "\n",
        "print(combined_dataset['Flow Bytes/s'].dtype)\n",
        "\n"
      ],
      "metadata": {
        "id": "7FwXtng89-qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separating features as X and target variable as Y\n",
        "X= combined_dataset.drop(columns=['Label'])\n",
        "Y= combined_dataset['Label']\n",
        "#print the shape of X and Y\n",
        "print(\"features shape\",X.shape)\n",
        "print(\"Target variable shape\", Y.shape)"
      ],
      "metadata": {
        "id": "K1BpInec-TT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#check for infinity values\n",
        "infVal=X.isin([np.inf, -np.inf]).sum()\n",
        "print(\"Infinity values in each column:\\n\", infVal[infVal>0])\n",
        "\n",
        "# Check for excessively large values (based on what you deem large)\n",
        "large_values = X.apply(lambda x: x > 1e6).sum()\n",
        "print(\"Large values in each column: \\n\", large_values[large_values > 0])"
      ],
      "metadata": {
        "id": "em8IR97pYomg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To Handle infinity values : Replace infinity values with\n",
        "X.replace([np.inf,-np.inf],np.nan,inplace=True)\n",
        "\n",
        "#To Handle Large values : Capping extremely large values\n",
        "X = X.apply(lambda x: np.where(x > 1e6, 1e6, x))  # Example threshold 1e6\n",
        "\n",
        "# Fill missing values after replacing infinity\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "# Check for any remaining missing values\n",
        "print(X.isnull().sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "UPC0uyvzbUmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting best features for the training model out of 78 features\n",
        "#use Random forest algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Train a model with Random Forest\n",
        "rfModel=RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfModel.fit(X,Y)\n",
        "\n",
        "#Get important feature\n",
        "importantFeatures=rfModel.feature_importances_"
      ],
      "metadata": {
        "id": "MNLkofDccgZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating another Data frame with selected most importand features and their importance factor\n",
        "importantFeatures_df=pd.DataFrame({\n",
        "    'Feature' : X.columns,\n",
        "    'Importance': importantFeatures\n",
        "})\n",
        "\n",
        "#Sort the Dataframe by importance scores in decending order\n",
        "importantFeatures_df = importantFeatures_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the top important features\n",
        "print(importantFeatures_df.head(20))  # Change 10 to the number of features you want to display\n",
        "\n",
        "# Plotting feature importances\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "plt.barh(importantFeatures_df['Feature'][:20], importantFeatures_df['Importance'][:20], color='skyblue')  # Change to top 20\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Top 20 Important Features')  # Update title to reflect the change\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EKeMOBwwiCVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importantFeatures_df.info()"
      ],
      "metadata": {
        "id": "t4z5mPXDtNyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'top_features' contains the names of the 20 important features\n",
        "top_features = importantFeatures_df.nlargest(20, 'Importance')['Feature']\n",
        "\n",
        "# Create a new DataFrame with selected features and the target variable\n",
        "selected_features_df = combined_dataset[top_features.values.tolist() + ['Label']]\n",
        "\n",
        "# Display the new DataFrame to verify\n",
        "print(selected_features_df.head())\n"
      ],
      "metadata": {
        "id": "XQs-d1Zhukbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features_df.info()"
      ],
      "metadata": {
        "id": "3X4CXNgZ0OII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#initiate the lable encoder\n",
        "labelEncoder=LabelEncoder()\n",
        "\n",
        "#fit the encoder to the lable data\n",
        "y_encode=labelEncoder.fit_transform(Y)\n",
        "\n",
        "#check the data type\n",
        "y_encode.dtype"
      ],
      "metadata": {
        "id": "AB5RqUEJ1bSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Use features from selected_features_df dataframe and y_encode\n",
        "x_train,x_test,y_train,y_test=train_test_split(selected_features_df,y_encode,test_size=0.2,random_state=42,stratify=y_encode)\n",
        "\n",
        "#check the shapes of the resulting datsets\n",
        "print(\"Traning features set shape:\",x_train.shape)\n",
        "print(\"Test features set shape:\",x_test.shape)\n",
        "print(\"Traning Lable set shape:\",y_train.shape)\n",
        "print(\"Test Lable set shape:\",y_test.shape)"
      ],
      "metadata": {
        "id": "gkKo5E945x3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(y_train))\n",
        "unique_values = pd.Series(y_train).unique()\n",
        "print(unique_values)\n"
      ],
      "metadata": {
        "id": "9NRD5iwu2_UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the class distribution | use the traning lable set\n",
        "unique, counts=np.unique(y_train,return_counts=True)\n",
        "class_distribution_training=dict(zip(unique,counts))#represent the different classes in the dataset as a dictionary\n",
        "print(class_distribution_training)\n",
        "\n",
        "\n",
        "#plot the class distribution\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(class_distribution_training.keys(),class_distribution_training.values(),color=\"blue\")\n",
        "plt.title('Training Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vb11gKMTjq8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use SMOTE for handle class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#Apply SMOTE to training features and traing lables\n",
        "#Create an instance of SMOTE\n",
        "smote=SMOTE(random_state=42)\n",
        "\n",
        "#Fit SMOTE to thr training data\n",
        "x_resampled, y_resampled = smote.fit_resample(x_train, y_train)\n",
        "\n",
        "#check the new class distribution\n",
        "print(\"Original traning labled distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\n Resampled training lables distribution:\")\n",
        "print(pd.Series(y_resampled).value_count())\n"
      ],
      "metadata": {
        "id": "FGuyh78AtOXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}