{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2a5af-eb28-4d21-97ea-b3abd210ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install imbalanced-learn\n",
    "!pip install scikit-learn\n",
    "!pip install gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51752f26-5e7f-451a-a574-3f4d3e3f2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "dataset_paths = [\n",
    "    'C:/Jupyter notebook/dataset_1.csv',\n",
    "    'C:/Jupyter notebook/dataset_2.csv',\n",
    "    'C:/Jupyter notebook/dataset_3.csv',\n",
    "    'C:/Jupyter notebook/dataset_4.csv',\n",
    "    'C:/Jupyter notebook/dataset_5.csv',\n",
    "    'C:/Jupyter notebook/dataset_6.csv',\n",
    "    'C:/Jupyter notebook/dataset_7.csv',\n",
    "    'C:/Jupyter notebook/dataset_8.csv',\n",
    "]\n",
    "\n",
    "# Check if files exist\n",
    "for path in dataset_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {path}\")\n",
    "\n",
    "# If all files are confirmed to exist, read them\n",
    "dataframes = [pd.read_csv(path) for path in dataset_paths if os.path.exists(path)]\n",
    "\n",
    "# Combine all datasets into one dataframe\n",
    "combined_dataset = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b8843-153c-4039-ab2a-adcb5c288aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6732fb-0c3f-40e4-8cc2-29c60a3017a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the dataset\n",
    "print(combined_dataset.head())\n",
    "print(combined_dataset.info())  # Review structure, data types, and non-null counts\n",
    "print(combined_dataset.describe())  # Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cd2f5-ca9f-445b-b0ee-285110710d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and check for missing values, data types\n",
    "missing_values = combined_dataset.isnull().sum()\n",
    "print(\"Missing values in each feature:\\n\", missing_values)\n",
    "\n",
    "# Checking data types and columns\n",
    "print(combined_dataset.dtypes)\n",
    "\n",
    "# Summary statistics\n",
    "print(combined_dataset.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea9e3a-d2aa-40a6-8b05-8b8e93b06ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a151e7-4dea-4806-9b43-e7356d9750e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target class (assuming 'Label' is the target column)\n",
    "print(combined_dataset[' Label'].value_counts())\n",
    "\n",
    "# Identifying the minority classes\n",
    "minority_classes = combined_dataset[' Label'].value_counts()[combined_dataset[' Label'].value_counts() < 1000]\n",
    "print(\"Minority attack classes:\\n\", minority_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccbfd35-8754-4815-8a86-11ad6e17abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (option to impute or drop)\n",
    "# Dropping rows with missing values\n",
    "combined_dataset = combined_dataset.dropna()\n",
    "\n",
    "# Remove duplicate records\n",
    "combined_dataset = combined_dataset.drop_duplicates()\n",
    "\n",
    "# Checking if duplicates were removed\n",
    "print(\"Data shape after removing duplicates:\", combined_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb8c50-a666-4389-9e65-21dd4fe60a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Automatically detect numerical columns\n",
    "numerical_cols = combined_dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN values in dataset:\\n\", combined_dataset[numerical_cols].isna().sum())\n",
    "\n",
    "# Check for infinity values\n",
    "print(\"Infinity values in dataset:\\n\", np.isinf(combined_dataset[numerical_cols]).sum())\n",
    "\n",
    "# Fill NaN with the mean of each column\n",
    "combined_dataset[numerical_cols] = combined_dataset[numerical_cols].fillna(combined_dataset[numerical_cols].mean())\n",
    "\n",
    "# Replace infinity with a large finite number\n",
    "combined_dataset[numerical_cols] = combined_dataset[numerical_cols].replace([np.inf, -np.inf], np.nan)\n",
    "combined_dataset[numerical_cols] = combined_dataset[numerical_cols].fillna(combined_dataset[numerical_cols].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34c94b-a843-4463-8b08-d962bd8e462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix X and target variable y\n",
    "X = combined_dataset.drop('Label', axis=1)  # Drop target column\n",
    "y = combined_dataset['Label']  # Target variable\n",
    "\n",
    "# Encode the labels (target variable) using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Encode target labels\n",
    "\n",
    "# Scale the features (if scaling is needed)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Standardize the feature set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71004d3-c8c4-4db6-9591-9a415a00dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMote(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y_encoded)\n",
    "\n",
    "# Check the new class distribution after SMOTE\n",
    "new_class_distribution = pd.Series(y_resampled).value_counts()\n",
    "print(\"New Class Distribution after SMOTE:\\n\", new_class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1db8d2-ec9c-4835-b01f-ea434a9ae1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the SMOTE-resampled data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training and Test sets created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85c149-a530-40b3-b29a-09218d77d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the SMOTE-resampled training data with all 80 features\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest model trained on all features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ce15e-2481-4158-b6f3-7bcc7e3cc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance scores from the trained model\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "feature_importances_df = pd.DataFrame({'Feature': combined_dataset.columns[:-1],  # Exclude 'Label'\n",
    "                                       'Importance': importances})\n",
    "\n",
    "# Sort features by importance in descending order and select the top 20 features\n",
    "top_20_features = feature_importances_df.sort_values(by='Importance', ascending=False).head(20)['Feature'].values\n",
    "print(\"Top 20 most important features:\\n\", top_20_features)\n",
    "\n",
    "# Create a new dataset with only the top 20 features\n",
    "X_resampled_top_20 = pd.DataFrame(X_resampled, columns=combined_dataset.columns[:-1])[top_20_features].values\n",
    "X_train_top_20, X_test_top_20, y_train_top_20, y_test_top_20 = train_test_split(X_resampled_top_20, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd370457-ce26-4b31-9383-484b20a046f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the Random Forest model using only the top 20 most important features\n",
    "rf_model_top_20 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_top_20.fit(X_train_top_20, y_train_top_20)\n",
    "\n",
    "print(\"Random Forest model retrained using top 20 features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7775919-3d58-4e82-a031-ddd973077b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on the test set using the Random Forest model trained with the top 20 features\n",
    "y_pred_top_20 = rf_model_top_20.predict(X_test_top_20)\n",
    "\n",
    "# Print classification report (Precision, Recall, F1-score)\n",
    "print(\"Classification Report with Top 20 Features:\\n\", classification_report(y_test_top_20, y_pred_top_20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d370028-0287-42f0-bd9b-912b4fc0a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate confusion matrix to evaluate true vs predicted classes\n",
    "conf_matrix_top_20 = confusion_matrix(y_test_top_20, y_pred_top_20)\n",
    "print(\"Confusion Matrix with Top 20 Features:\\n\", conf_matrix_top_20)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_top_20, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix with Top 20 Features')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bb4e7-8e55-4e3f-b8eb-f5e3392f1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Compute the ROC-AUC score for multi-class classification (One-vs-Rest)\n",
    "roc_score_top_20 = roc_auc_score(y_test_top_20, rf_model_top_20.predict_proba(X_test_top_20), multi_class='ovr')\n",
    "print(f\"ROC-AUC Score with Top 20 Features: {roc_score_top_20:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
